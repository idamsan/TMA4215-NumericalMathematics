{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMA4215 Numerisk Matematikk \n",
    "\n",
    "Høst 2021 – September 10, 2021\n",
    "\n",
    "R. Bergmann, E. Çokaj, O. P. Hellan \n",
    "\n",
    "# Assignment 2\n",
    "\n",
    "## Deadline\n",
    "September 17, 2021, 23:59\n",
    "\n",
    "\n",
    "## Submission\n",
    "submit your Jupyter notebook containing the solution via upload in blackboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Let $A\\in \\mathbb R^{n\\times n}$ denote an invertible $n$-by-$n$ dimensional matrix.\n",
    "\n",
    "Compute the following gradients $\\nabla f$ and Jacobi matrices $J_F$, respectively. At which points is the gradient or Jacobi matrix welldefined? \n",
    "1. $f_1\\colon \\mathbb R^n \\to \\mathbb R$ with $f_1(\\mathbf{x}) = \\lVert A\\mathbf{x} \\rVert_2^2$ \n",
    "2. $f_2\\colon \\mathbb R^n \\to \\mathbb R$ with $f_2(\\mathbf{x}) = \\lVert \\mathbf x \\rVert$\n",
    "3. $f_3\\colon \\mathbb R^n \\to \\mathbb R$ with $f_3(\\mathbf{x}) = (A\\mathbf{x}, \\mathbf{x})$, where $(\\cdot,\\cdot)$ denotes the usual inner product. How does this simplify if $A$ is symmetric?\n",
    "4. $F_4\\colon \\mathbb R^n \\to \\mathbb R^n$ with $F_4(\\mathbf{x}) = A\\mathbf{x}$\n",
    "\n",
    "Further compute the Hessian Matrix $\\nabla^2 f$ of the function $f_3$.\n",
    "\n",
    "_Bonus question_: Compute the gradient for $F_5\\colon\\mathbb R^n\\to\\mathbb R$, $F_5(\\mathbf x) = \\lVert A\\mathbf x - \\mathbf b\\rVert_2^2$ for a rectangular matrix $A\\in\\mathbb R^{m\\times n}$, $m\\geq n$, and a vector $\\mathbf b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1**\n",
    "\n",
    "$||A\\vec{x}||_2^2= \\sum_{i=1}^n{|(A\\vec{x})_i|}^2 = \\sum_{i=1}^n{(\\sum_{j=1}^n{a_{ij}x_j}})^2$.\n",
    "\n",
    "To compute the gradient we need to differentiate this with regards to all $x_i$. I will compute in regards to a arbitrary $x_k$, and then see what it will look like for all $x_i$. I will use this method for 1.2, 1.3 and 1.4 as well. \n",
    "\n",
    "$\\frac{\\partial}{\\partial{x_k}}(\\sum_{i=1}^n{(\\sum_{j=1}^n{a_{ij}x_j}})^2) = $\n",
    "\n",
    "$\\sum_{i=1}^n{2a_{ik}(\\sum_{j=1}^n{a_{ij}x_j}})= $\n",
    "\n",
    "$\\sum_{i=1}^n2a_{ik}(A\\vec{x})_i=$\n",
    "\n",
    "$2a_{1k}(A\\vec{x})_1 + 2a_{2k}(A\\vec{x})_2+...+2a_{nk}(A\\vec{x})_n = $\n",
    "\n",
    "$(2A^TA\\vec{x})_k$\n",
    "\n",
    "$\\implies \\nabla f_1(\\vec{x})=2A^TA\\vec{x} $ \n",
    "\n",
    "The gradient is well-defined for all $\\vec{x}\\in \\mathbb{R^n}$.\n",
    "\n",
    "**1.2**\n",
    "\n",
    "Assuming $||\\cdot||_2$.\n",
    "\n",
    "$||\\vec{x}||_2 = (\\sum_{i=1}^n |x_i|^2)^\\frac{1}{2}$.\n",
    "\n",
    "$\\frac{\\partial}{\\partial{x_k}}(\\sum_{i=1}^n |x_i|^2)^\\frac{1}{2}=$\n",
    "\n",
    "$\\frac{1}{2}2x_k(\\sum_{i=1}^n |x_i|^2)^\\frac{-1}{2}=$\n",
    "\n",
    "$x_k(||\\vec{x}||_2^2)^\\frac{-1}{2}$\n",
    "\n",
    "$\\implies \\nabla f_2(\\vec{x}) = \\frac{\\vec{x}}{||\\vec{x}||_2}$\n",
    "\n",
    "The gradient is well-defined for all $\\vec{x}\\in \\mathbb{R^n} \\backslash \\vec{0}$\n",
    "\n",
    "**1.3**\n",
    "\n",
    "$(A\\vec{x},\\vec{x})=\\sum_{i=1}^n(A\\vec{x})_ix_i=\\sum_{i=1}^nx_i(\\sum_{j=1}^n a_{ij}x_j)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_k}(\\sum_{i,j=1}^nx_ia_{ij}x_j )=$\n",
    "\n",
    "$\\sum_{j=1}^n a_{kj}x_j + \\sum_{i=1}^n x_i a_{ik}=$\n",
    "\n",
    "$[(A^T + A)\\vec{x}]_k$\n",
    "\n",
    "$\\implies \\nabla f_3(\\vec{x}) = (A^T+A)\\vec{x} $\n",
    "\n",
    "If A is symmetric this simplifies to $2A\\vec{x}$.\n",
    "\n",
    "The gradient is well-defined for all $\\vec{x}\\in \\mathbb{R^n}$.\n",
    "\n",
    "**1.4**\n",
    "\n",
    "$A\\vec{x}=\\sum_{i,j=1}^na_{ij}x_j$.\n",
    "\n",
    "Say $\\vec{F_4}=[F_1, F_2,..., F_n]^T$. To compute the Jacobi matrix I can for example look at the gradient for each $F_i$.\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_k}(\\sum_{i,j=1}^na_{ij}x_j)=\\sum_{i=1}^na_{ik}=$ column k.\n",
    "\n",
    "$\\implies J_f(\\vec{x}) = A.$\n",
    "\n",
    "The Jacobi matrix is well-defined for all $\\vec{x}\\in \\mathbb{R^n}$.\n",
    "\n",
    "**The Hessian matrix for $f_3$**\n",
    "\n",
    "$\\nabla f_3 = A\\vec{x} + A^T\\vec{x}$. From 1.4 we Know how to compute $\\nabla^2f_3$.\n",
    "\n",
    "$\\nabla^2f_3=A+A^T$ (2A if A symmetric).\n",
    "\n",
    "**Bonus question**\n",
    "\n",
    "$||A\\vec{x}-\\vec{b}||_2^2 = \\sum_{i=1}^n|A\\vec{x}-\\vec{b}|_i^2 =$\n",
    "\n",
    "$\\sum_{i=1}^n(A\\vec{x}-\\vec{b})^T(A\\vec{x}-\\vec{b})=$\n",
    "\n",
    "$((A\\vec{x})^T-\\vec{b}^T)(A\\vec{x}-\\vec{b})=$\n",
    "\n",
    "$(A\\vec{x})^T(A\\vec{x})-(A\\vec{x})^T\\vec{b}-\\vec{b}^T(A\\vec{x})+\\vec{b}^T\\vec{b}=$\n",
    "\n",
    "$||A\\vec{x}||^2 - (A\\vec{x},b) - (b,A\\vec{x}) +\\vec{b}^T\\vec{b}=$\n",
    "\n",
    "$||A\\vec{x}||^2 - 2\\vec{b}^TA\\vec{x} +\\vec{b}^T\\vec{b}=$\n",
    "\n",
    "$\\frac{\\partial}{\\partial x_k}(\\sum_{i=1}^n|A\\vec{x}-\\vec{b}|_i^2)=$\n",
    "\n",
    "$ \\frac{\\partial}{\\partial x_k}(||A\\vec{x}||^2 - 2\\vec{b}^TA\\vec{x} +\\vec{b}^T\\vec{b})=$\n",
    "\n",
    "$(2A^TA\\vec{x})_k-(2A^T\\vec{b})_k$\n",
    "\n",
    "$\\implies \\nabla F_5(\\vec{x})=2A^TA\\vec{x}-2A^T\\vec{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "We consider the linear system of equations\n",
    "$A\\mathbf{x}=\\mathbf{b}$, where\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "   3 & -1 & -c\\\\\n",
    "  -1 &  3 &  0\\\\\n",
    "  -c &  0 &  3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "for some $c\\in \\mathbb R$.\n",
    "\n",
    "1. Consider the Jacobi Method. What does the iteration matrix $B_{\\mathrm{J}}$ look like?\n",
    "2. For which parameters $c \\in \\mathbb R$ does the Jacobi method converge?\n",
    "    \n",
    "    _Hint:_ Consider the eigenvalues of $B_{\\mathrm{J}}$.\n",
    "3. Prove that the Gauss-Seidel Method converges for any $c \\in [-1,1]$.  \n",
    "    \n",
    "    _Hint:_ you may use the property, that the Gauss-Seidel method converges if $A$ is strictly diagonal dominant, i.e. if\n",
    "      $\\lvert a_ii \\rvert > \\displaystyle\\sum_{i\\neq j} \\lvert a_{ij} \\rvert$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1**\n",
    "\n",
    "$B_j=D^{-1}(E+F)=$\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "   1/3 & 0 & 0\\\\\n",
    "  0 &  1/3 &  0\\\\\n",
    "  0 &  0 &  1/3\n",
    "\\end{pmatrix}\n",
    "\\left(\n",
    "\\begin{pmatrix}\n",
    "   0 & 0 & 0\\\\\n",
    "  1 &  0 &  0\\\\\n",
    "  c &  0 &  0\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "   0 & 1 & c\\\\\n",
    "  0 &  0 &  0\\\\\n",
    "  0 &  0 &  0\n",
    "\\end{pmatrix}\n",
    "\\right)\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "   0 & 1/3 & c/3\\\\\n",
    "  1/3 &  0 &  0\\\\\n",
    "  c/3 &  0 &  0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**2.2**\n",
    "The Jacobi method converges when the spectral radius is less than 1. We can find c by finding the eigenvalues and force them to be less than 1. \n",
    "\n",
    "$det(A-I\\lambda) = -\\lambda^3+\\lambda/9+\\lambda c^2/9=0$\n",
    "\n",
    "$\\implies \\lambda=\\pm \\sqrt{{(1-c^2)}/9}$\n",
    "\n",
    "$\\lambda < 1 \\implies c\\in(-\\sqrt{8},\\sqrt{8})$\n",
    "\n",
    "**2.3**\n",
    "Using the hint it is clear that |c| must be less than 2, and |c|<1 is in that interval. So the method must converge for all $c\\in (-1,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "Consider the matrix $A\\in \\mathbb R^{n\\times n}$ of the form $a_{ij} = 3^{-\\lvert i-j\\rvert} + 2^{-i-j} + 10^{-6} \\cdot$ `randn()`, where `randn()` refers to a random number per entry (independently) normal distributed with mean `0` and variance `1`, cf. `numpy.random.randn`. Further let $\\mathbf{b} = (1,\\ldots,1)^\\mathrm{T} \\in \\mathbb R^{n}$.\n",
    "\n",
    "We want to compare a few algorithms. Implement one method\n",
    "\n",
    "```\n",
    "    method_name(A,b,x0,tol,maxiter)\n",
    "```\n",
    "\n",
    "For each of the following three or four methods (the fourth is optional), where the `method_name` is given in brackets, but they should all follow the same interface.\n",
    "\n",
    "__Richardson iteration__ (`richardson(...)`)\n",
    "\n",
    "Repeat for \\(k=0,...\\) until the stopping criterion is fulfilled\n",
    "\n",
    "$$\\begin{split}\n",
    "\\mathbf{r}^{(k)} &= \\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(k)},\\\\\n",
    "\\mathbf{x}^{(k+1)} &= \\mathbf{x}^{(k)} + \\omega \\mathbf{r^{(k)}}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where $\\omega$ is a sixth (optional) parameter to the function `richardson()`.\n",
    "\t\t\n",
    "\n",
    "__Gauß-Seidel Method__ (`gaussseidel(...)`)\n",
    "\n",
    "Repeat for \\(k=0,...\\) until the stopping criterion is fulfilled\n",
    "\n",
    "$$x_j^{(k+1)} = \\frac{1}{a_{jj}}\\left(b_j -\\sum_{i=1}^{j-1}a_{ji}x_i^{(k+1)}-\\sum_{i=j+1}^n a_{ji}x_i^{(k)}\\right),\\quad j=1,\\ldots,n.$$\n",
    "\n",
    "\n",
    "__Gradient Descent__ (`steepestdescent(...)`)\n",
    "\n",
    "Initialize\n",
    "\n",
    "$$\\mathbf{r}^{(0)} =\t\\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(0)}$$\n",
    "\n",
    "Repeat for \\(k=0,...\\) until the stopping criterion is fulfilled\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\t\t\\omega_k\n",
    "\t\t&=\n",
    "\t\t\\frac{(\\mathbf{r}^{(k)})^\\mathrm{T}\\mathbf{r}^{(k)}}{(\\mathbf{r}^{(k)})^\\mathrm{T}\\mathbf{A}\\mathbf{r}^{(k)}},\n",
    "\t\t\\\\\n",
    "\t\t\\mathbf{x}^{(k+1)}\n",
    "\t\t&=\n",
    "\t\t\\mathbf{x}^{(k)} + \\omega_k\\mathbf{r}^{(k)},\n",
    "\t\t\\\\\n",
    "\t\t\\mathbf{r}^{(k+1)}\n",
    "\t\t&=\n",
    "\t\t\\mathbf{r}^{(k)} - \\omega_k\\mathbf{A}\\mathbf{r}^{(k)}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "__Conjugate Gradient Iteration__ (`cg(...)`) (_Bonus Algorithm_)\n",
    "\n",
    "Initialize\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{v}^{(0)}\n",
    "=\n",
    "\\mathbf{r}^{(0)}\n",
    "&=\n",
    "\\mathbf{b}-\\mathbf{A}\\mathbf{x}^{(0)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Repeat for \\(k=0,...\\) until the stopping criterion is fulfilled\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\omega_k &=\n",
    "\t\t\\frac{(\\mathbf{r}^{(k)})^\\mathrm{T}\\mathbf{r}^{(k)}}{(\\mathbf{v}^{(k)})^\\mathrm{T}\\mathbf{A}\\mathbf{v}^{(k)}},\n",
    "\t\t\\\\\n",
    "\t\t\\mathbf{x}^{(k+1)}\n",
    "\t\t&=\n",
    "\t\t\\mathbf{x}^{(k)} + \\omega_k\\mathbf{v}^{(k)},\n",
    "\t\t\\\\\n",
    "\t\t\\mathbf{r}^{(k+1)}\n",
    "\t\t&=\n",
    "\t\t\\mathbf{r}^{(k)} - \\omega_k\\mathbf{A}\\mathbf{v}^{(k)},\n",
    "\t\t\\\\\n",
    "\t\t\\beta_k\n",
    "\t\t&=\n",
    "\t\t\\frac{(\\mathbf{r}^{(k+1)})^\\mathrm{T}\\mathbf{r}^{(k+1)}}{(\\mathbf{r}^{(k)})^\\mathrm{T}\\mathbf{r}^{(k)}},\n",
    "\t\t\\\\\n",
    "\t\t\\mathbf{v}^{(k+1)}\n",
    "\t\t&=\n",
    "\t\t\\mathbf{r}^{(k+1)} + \\beta_k\\mathbf{v}^{(k)}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "All should stop if _either_ the relative error $\\frac{\\lVert \\mathbf{r}^{(k)} \\rVert}{\\lVert \\mathbf{r}^{(0)} \\rVert} < \\varepsilon$, where $\\varepsilon$ is our `tol`erance  _or_ a maximum number of iterattions `maxiter` is reached; in your experiments, use \n",
    "\n",
    "* matrix size `n=2000`\n",
    "* `tol=10^{-7}`\n",
    "* `maxiter=5000`.\n",
    "\n",
    "We always start with $\\mathbf{x}^{(0)} = \\mathbf b$.\n",
    "\n",
    "We are interested in a comparison of __runtime__, __number of iterations__ and __final error__ for each of the four methods. Either provide this automatically with your code or provide the results in a Markdown table (copied from your measurements in the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "#Creating the matrix for any n\n",
    "def A(n):\n",
    "    a=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            a[i][j]=3**(-abs(i-j))+2**(-i-j)+10**(-6)*np.random.randn()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Richardson \n",
    "def richardson(A,b,x0,tol,maxiter,w):\n",
    "    r0=b-A@x0  #@ matrix multiplication\n",
    "    x=x0+w*r0\n",
    "    iters=1    #One iteration already done \n",
    "    error=1\n",
    "    while iters<maxiter and error>tol:\n",
    "        r_next=b-A@x\n",
    "        x=x+w*r_next\n",
    "        error=np.linalg.norm(r_next,2)/np.linalg.norm(r0,2)\n",
    "        iters+=1\n",
    "        \n",
    "    return iters, error, np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gauss-Seidel \n",
    "def gauss_seidel(A,b,x0,tol,maxiter):\n",
    "    n=A.shape[0]\n",
    "    r0=b-A@x0\n",
    "    iters=0\n",
    "    error=1\n",
    "    x_0 = x0.copy()\n",
    "    x = x_0\n",
    "    while iters < maxiter and error> tol:\n",
    "        for j in range(0,n):\n",
    "            d=b[j]\n",
    "            for i in range(0,n):\n",
    "                if(j!=i):\n",
    "                    d-=(A[j,i]*x[i])\n",
    "            x[j]=d/A[j,j]\n",
    "        r=b-A@x\n",
    "        error = np.linalg.norm(r,2)/np.linalg.norm(r0,2)    \n",
    "        iters+=1\n",
    "            \n",
    "                \n",
    "    return iters, error, np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "def gradientDescent(A,b,x0,tol,maxiter):\n",
    "    r0=b-A@x0\n",
    "    rk=r0\n",
    "    xk=x0\n",
    "    iters=0\n",
    "    error=1\n",
    "    while iters<maxiter and error>tol:\n",
    "        wk=rk.T*rk/(rk.T*A@rk)\n",
    "        xk=xk+wk*rk\n",
    "        rk=rk-wk*A@rk\n",
    "        iters+=1\n",
    "        error = np.linalg.norm(rk)/np.linalg.norm(r0) \n",
    "    return iters, error, np.linalg.norm(xk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjugate Gradient (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding runtimes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 132 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29, 9.26688981259543e-08, 22.358758383316523)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "richardson(B, b, b, 10**(-7), 5000, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15, 6.905283680425114e-08, 22.358756881327835)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gauss_seidel(B,b,b,10**(-7),5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 715 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 7.603418048443178e-08, 22.35875837226542)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gradientDescent(B,b,b,10**(-7),5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the different methods:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for running/comparison\n",
    "B = A(2000)\n",
    "b = np.ones(2000)\n",
    "\n",
    "it_r, error_r, x_norm_r = richardson(B, b, b, 10**(-7), 5000, 0.5)\n",
    "it_gs, error_gs, x_norm_gs = gauss_seidel(B,b,b,10**(-7),5000)\n",
    "it_gd, error_gd, x_norm_gd = gradientDescent(B,b,b,10**(-7),5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richardson iteration gives  29  iterations and error:  9.267505253978565e-08 . The runtime is shown above.\n",
      "----------------------------------------------------------------------------------------------\n",
      "The Gauss-Seidel method gives  15  iterations and error:  6.905357238976052e-08 . The runtime is shown above.\n",
      "----------------------------------------------------------------------------------------------\n",
      "The gradient descent method gives  20  iterations and error:  7.670046435638508e-08 . The runtime is shown above.\n",
      "\n",
      "\n",
      "The Richardson iteration has the smallest runtime, but the biggest error and the most iterations needed for convergence.\n",
      "The Gauss-Seidel method has the smallest error and the smallest amount of iterations, but has by far the longest runtime.\n",
      "The gradient descent method is in the middle in all areas.\n"
     ]
    }
   ],
   "source": [
    "print(\"Richardson iteration gives \", it_r, \" iterations and error: \", error_r, \". The runtime is shown above.\")\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"The Gauss-Seidel method gives \", it_gs, \" iterations and error: \", error_gs, \". The runtime is shown above.\")\n",
    "print(\"----------------------------------------------------------------------------------------------\")\n",
    "print(\"The gradient descent method gives \", it_gd, \" iterations and error: \", error_gd, \". The runtime is shown above.\")\n",
    "print(\"\\n\")\n",
    "print(\"The Richardson iteration has the smallest runtime, but the biggest error and the most iterations needed for convergence.\")\n",
    "print(\"The Gauss-Seidel method has the smallest error and the smallest amount of iterations, but has by far the longest runtime.\")\n",
    "print(\"The gradient descent method is in the middle in all areas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
